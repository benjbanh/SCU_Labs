{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook Demo: Approximating Cosine Similarity based on the Cauchy-Schwarz Inequality\n",
    "\n",
    "This notebook demonstrates a technique to approximate the cosine similarity\n",
    "between two L2-normalized vectors by splitting them at a random feature\n",
    "and calculating the dot product of the prefix and the product of the L2 norms\n",
    "of the suffix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "Cosine similarity is a common measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. For L2-normalized vectors $a$ and $b$, the cosine similarity is simply their dot product: $\\cos(a, b) = a \\cdot b$.\n",
    "\n",
    "This notebook explores an approximation technique for the cosine similarity. Given two $m$-dimensional L2-normalized vectors $a$ and $b$, we can choose a random feature index $f$ (where $1 \\le f \\le m-1$) and approximate the cosine similarity using the following inequality:\n",
    "\n",
    "$$\\cos(a, b) \\le a^{<f} \\cdot b^{<f} + ||a^{\\ge f}||_2 ||b^{\\ge f}||_2$$\n",
    "\n",
    "where:\n",
    "* $a^{<f}$ is a vector with the first $f$ features of $a$ and all subsequent features set to 0, which we call the *prefix* vector.\n",
    "* $b^{<f}$ is similarly defined for vector $b$.\n",
    "* $a^{\\ge f}$ is a vector with the features from index $f$ to $m-1$ of $a$ and all preceding features set to 0, which we call the *sufix* vector.\n",
    "* $b^{\\ge f}$ is similarly defined for vector $b$.\n",
    "* $||v||_2$ denotes the L2 norm (Euclidean norm) of a vector $v$.\n",
    "\n",
    "This inequality is derived from the Cauchy-Schwarz inequality, which states that:\n",
    "\n",
    "$$a \\cdot b \\le ||a||_2 ||b||_2.$$\n",
    "\n",
    "Given that $a$ and $b$ are normalized vectors, the inequality does not help us much, as it simply states that the dot-product of the vectors (and by extension their cosine similarity) is smaller or equal to 1. Well, we already knew that...\n",
    "\n",
    "However, this approximation can be useful in certain scenarios, especially when dealing with high-dimensional data, sparse data, or when computational efficiency is a concern. This notebook will demonstrate this concept with randomly generated L2-normalized vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def l2_normalize(vector):\n",
    "    \"\"\"L2-normalizes a given vector.\"\"\"\n",
    "    norm_val = norm(vector)\n",
    "    if norm_val == 0:\n",
    "        return vector\n",
    "    return vector / norm_val\n",
    "\n",
    "def approximate_cosine_similarity(a, b, f):\n",
    "    \"\"\"\n",
    "    Approximates the cosine similarity between two L2-normalized vectors\n",
    "    using a random feature split.\n",
    "\n",
    "    Args:\n",
    "        a (np.array): The first L2-normalized vector.\n",
    "        b (np.array): The second L2-normalized vector.\n",
    "        f (int): The random feature index (1 <= f <= len(a) - 1).\n",
    "\n",
    "    Returns:\n",
    "        float: The upper bound approximation of the cosine similarity.\n",
    "    \"\"\"\n",
    "    m = len(a)\n",
    "    if not (1 <= f <= m - 1):\n",
    "        raise ValueError(f\"Feature index f must be between 1 and {m-1} (inclusive). Got {f}.\")\n",
    "\n",
    "    a_prefix = np.concatenate((a[:f], np.zeros(m - f)))\n",
    "    b_prefix = np.concatenate((b[:f], np.zeros(m - f)))\n",
    "\n",
    "    a_suffix = np.concatenate((np.zeros(f), a[f:]))\n",
    "    b_suffix = np.concatenate((np.zeros(f), b[f:]))\n",
    "\n",
    "    dot_product_prefix = np.dot(a_prefix, b_prefix)\n",
    "    norm_product_suffix = norm(a_suffix) * norm(b_suffix)\n",
    "\n",
    "    return dot_product_prefix + norm_product_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Cosine Similarity: 0.7210\n",
      "Randomly chosen feature index f: 24\n",
      "Approximated Cosine Similarity (upper bound): 0.9269\n",
      "The inequality holds: cos(a, b) <= dot(a^<f, b^<f) + ||a^>=f||_2 ||b^>=f||_2\n"
     ]
    }
   ],
   "source": [
    "# Set the dimension of the vectors\n",
    "m = 100\n",
    "\n",
    "# Generate two random vectors\n",
    "np.random.seed(42)  # for reproducibility\n",
    "vector_a_raw = np.random.rand(m)\n",
    "vector_b_raw = np.random.rand(m)\n",
    "\n",
    "# L2-normalize the vectors\n",
    "vector_a = l2_normalize(vector_a_raw)\n",
    "vector_b = l2_normalize(vector_b_raw)\n",
    "\n",
    "# Calculate the actual cosine similarity\n",
    "cosine_similarity_actual = np.dot(vector_a, vector_b)\n",
    "print(f\"Actual Cosine Similarity: {cosine_similarity_actual:.4f}\")\n",
    "\n",
    "# Choose a random feature index f\n",
    "f = np.random.randint(1, m)\n",
    "print(f\"Randomly chosen feature index f: {f}\")\n",
    "\n",
    "# Approximate the cosine similarity\n",
    "cosine_similarity_approx = approximate_cosine_similarity(vector_a, vector_b, f)\n",
    "print(f\"Approximated Cosine Similarity (upper bound): {cosine_similarity_approx:.4f}\")\n",
    "\n",
    "# Verify the inequality\n",
    "if cosine_similarity_actual <= cosine_similarity_approx + 1e-9: # Adding a small tolerance for floating-point comparisons\n",
    "    print(\"The inequality holds: cos(a, b) <= dot(a^<f, b^<f) + ||a^>=f||_2 ||b^>=f||_2\")\n",
    "else:\n",
    "    print(\"The inequality DOES NOT hold (potential issue in implementation or theory understanding).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploring the Impact of the Split Point**\n",
    "\n",
    "Let's explore how the choice of the random feature split $f$ can affect the tightness of the upper bound. We will calculate the approximation for several different random values of $f$ and compare them to the actual cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exploring different random feature splits:\n",
      "  Split f = 75: Approximated Cosine Similarity = 0.7852\n",
      "  Split f = 72: Approximated Cosine Similarity = 0.7959\n",
      "  Split f = 36: Approximated Cosine Similarity = 0.8970\n",
      "  Split f = 38: Approximated Cosine Similarity = 0.8908\n",
      "  Split f = 84: Approximated Cosine Similarity = 0.7502\n",
      "\n",
      "Actual Cosine Similarity (for reference): 0.7210\n"
     ]
    }
   ],
   "source": [
    "# Number of random splits to test\n",
    "num_splits = 5\n",
    "\n",
    "print(\"\\nExploring different random feature splits:\")\n",
    "for i in range(num_splits):\n",
    "    f_random = np.random.randint(1, m)\n",
    "    approx_similarity = approximate_cosine_similarity(vector_a, vector_b, f_random)\n",
    "    print(f\"  Split f = {f_random}: Approximated Cosine Similarity = {approx_similarity:.4f}\")\n",
    "\n",
    "print(f\"\\nActual Cosine Similarity (for reference): {cosine_similarity_actual:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous demo, we explored the inequality:\n",
    "\n",
    "$$\\cos(a, b) \\le a^{<f} \\cdot b^{<f} + ||a^{\\ge f}||_2 ||b^{\\ge f}||_2.$$\n",
    "\n",
    "Two other inequalities that also hold are:\n",
    "\n",
    "1.  $$\\cos(a, b) \\le a^{<f} \\cdot b^{<f} + ||a^{\\ge f}||_2$$\n",
    "2.  $$\\cos(a, b) \\le a^{<f} \\cdot b^{<f} + ||b^{\\ge f}||_2$$\n",
    "\n",
    "These assume that the other vector has all of their features in the suffix, thus its suffix norm would be 1. We will now generate random L2-normalized vectors and, for a chosen random feature split, calculate the upper bounds provided by all three inequalities and compare them to the actual cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Cosine Similarity: 0.7210\n",
      "Randomly chosen feature index f: 99\n",
      "Approximation (Version 1): 0.7210\n",
      "Approximation (Version 2): 0.7378\n",
      "Approximation (Version 3): 0.8536\n",
      "Inequality holds for Version 1.\n",
      "Inequality holds for Version 2.\n",
      "Inequality holds for Version 3.\n"
     ]
    }
   ],
   "source": [
    "def approximate_cosine_similarity_v2(a, b, f):\n",
    "    \"\"\"\n",
    "    Approximates cosine similarity: dot(a^<f, b^<f) + ||a^>=f||_2\n",
    "    \"\"\"\n",
    "    m = len(a)\n",
    "    if not (1 <= f <= m - 1):\n",
    "        raise ValueError(f\"Feature index f must be between 1 and {m-1} (inclusive). Got {f}.\")\n",
    "\n",
    "    a_prefix = np.concatenate((a[:f], np.zeros(m - f)))\n",
    "    b_prefix = np.concatenate((b[:f], np.zeros(m - f)))\n",
    "\n",
    "    a_suffix = np.concatenate((np.zeros(f), a[f:]))\n",
    "\n",
    "    dot_product_prefix = np.dot(a_prefix, b_prefix)\n",
    "    norm_suffix_a = norm(a_suffix)\n",
    "\n",
    "    return dot_product_prefix + norm_suffix_a\n",
    "\n",
    "def approximate_cosine_similarity_v3(a, b, f):\n",
    "    \"\"\"\n",
    "    Approximates cosine similarity: dot(a^<f, b^<f) + ||b^>=f||_2\n",
    "    \"\"\"\n",
    "    m = len(a)\n",
    "    if not (1 <= f <= m - 1):\n",
    "        raise ValueError(f\"Feature index f must be between 1 and {m-1} (inclusive). Got {f}.\")\n",
    "\n",
    "    a_prefix = np.concatenate((a[:f], np.zeros(m - f)))\n",
    "    b_prefix = np.concatenate((b[:f], np.zeros(m - f)))\n",
    "\n",
    "    b_suffix = np.concatenate((np.zeros(f), b[f:]))\n",
    "\n",
    "    dot_product_prefix = np.dot(a_prefix, b_prefix)\n",
    "    norm_suffix_b = norm(b_suffix)\n",
    "\n",
    "    return dot_product_prefix + norm_suffix_b\n",
    "\n",
    "# Test the new approximations\n",
    "# Calculate the actual cosine similarity\n",
    "cosine_similarity_actual = np.dot(vector_a, vector_b)\n",
    "print(f\"Actual Cosine Similarity: {cosine_similarity_actual:.4f}\")\n",
    "\n",
    "# Choose a random feature index f\n",
    "f = np.random.randint(1, m)\n",
    "print(f\"Randomly chosen feature index f: {f}\")\n",
    "\n",
    "# Approximate the cosine similarity using all three versions\n",
    "approx_similarity_v1 = approximate_cosine_similarity(vector_a, vector_b, f)\n",
    "approx_similarity_v2 = approximate_cosine_similarity_v2(vector_a, vector_b, f)\n",
    "approx_similarity_v3 = approximate_cosine_similarity_v3(vector_a, vector_b, f)\n",
    "\n",
    "print(f\"Approximation (Version 1): {approx_similarity_v1:.4f}\")\n",
    "print(f\"Approximation (Version 2): {approx_similarity_v2:.4f}\")\n",
    "print(f\"Approximation (Version 3): {approx_similarity_v3:.4f}\")\n",
    "\n",
    "# Verify the inequalities\n",
    "tolerance = 1e-9\n",
    "if cosine_similarity_actual <= approx_similarity_v1 + tolerance:\n",
    "    print(\"Inequality holds for Version 1.\")\n",
    "else:\n",
    "    print(\"Inequality DOES NOT hold for Version 1.\")\n",
    "\n",
    "if cosine_similarity_actual <= approx_similarity_v2 + tolerance:\n",
    "    print(\"Inequality holds for Version 2.\")\n",
    "else:\n",
    "    print(\"Inequality DOES NOT hold for Version 2.\")\n",
    "\n",
    "if cosine_similarity_actual <= approx_similarity_v3 + tolerance:\n",
    "    print(\"Inequality holds for Version 3.\")\n",
    "else:\n",
    "    print(\"Inequality DOES NOT hold for Version 3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "More examples of useful cosine similarity approximations can be found in the following papers by David C. Anastasiu:\n",
    "\n",
    "```bibtex\n",
    "@inproceedings{anastasiu-icde2014,\n",
    "   author    = {David C. Anastasiu and George Karypis},\n",
    "   title     = {L2AP: Fast Cosine Similarity Search With Prefix L-2 Norm Bounds},\n",
    "   booktitle = {The 30th IEEE International Conference on Data Engineering},\n",
    "   series    = {ICDE 2014},\n",
    "   year      = {2014},\n",
    "   pages     = {784-795},\n",
    "   location  = {Chicago, USA},\n",
    "}\n",
    "\n",
    "@inproceedings{anastasiu-cikm2015,\n",
    "   author    = {David C. Anastasiu and George Karypis},\n",
    "   title     = {L2Knng: Fast Exact K-Nearest Neighbor Graph Construction with L2-Norm Pruning},\n",
    "   booktitle = {Proceedings of the 24th ACM International Conference on Information and Knowledge Management},\n",
    "   series    = {CIKM'15},\n",
    "   year      = {2015},\n",
    "   pages     = {791-800},\n",
    "   publisher = {ACM},\n",
    "   address   = {New York, NY, USA},\n",
    "   location  = {Melbourne, Australia}\n",
    "}\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "342wi25",
   "language": "python",
   "name": "342wi25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
