{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 08: Association Rule Mining\n",
    "\n",
    "**Objective:** This lab aims to introduce the concept of association rule mining, a popular technique for discovering interesting relationships hidden in large datasets. We will explore how to use existing Python packages to perform association rule mining and interpret the results. Finally, you will apply these techniques to a new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Association Rule Mining\n",
    "\n",
    "Association rule mining is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness.\n",
    "\n",
    "The classic example is the \"market basket analysis,\" where a retailer tries to understand purchasing behaviors of customers. For example, an association rule might be: `{Diapers} -> {Beer}`. This rule suggests that customers who buy diapers also tend to buy beer.\n",
    "\n",
    "Key concepts in association rule mining include:\n",
    "\n",
    "* **Itemset:** A collection of one or more items. E.g., `{Milk, Bread, Diaper}`.\n",
    "* **Support:** The fraction of transactions that contain an itemset. It indicates the popularity of an itemset. \n",
    "    $$Support(X) = \\frac{\\text{Number of transactions containing X}}{\\text{Total number of transactions}}$$\n",
    "* **Confidence:** Measures how often items in Y appear in transactions that contain X. It indicates the likelihood of item Y being purchased when item X is purchased.\n",
    "    $$Confidence(X \\rightarrow Y) = \\frac{Support(X \\cup Y)}{Support(X)}$$\n",
    "* **Lift:** Measures how much more often X and Y occur together than expected if they were statistically independent. A lift greater than 1 suggests a positive association.\n",
    "    $$Lift(X \\rightarrow Y) = \\frac{Support(X \\cup Y)}{Support(X) \\times Support(Y)}$$\n",
    "* **Antecedent (LHS):** The itemset on the left-hand side of the rule (e.g., `{Diapers}`).\n",
    "* **Consequent (RHS):** The itemset on the right-hand side of the rule (e.g., `{Beer}`).\n",
    "\n",
    "The most common algorithm for association rule mining is the **Apriori algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup\n",
    "\n",
    "First, let's install and import the necessary Python libraries. We'll primarily use `pandas` for data manipulation and `mlxtend` for association rule mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install mlxtend if you haven't already\n",
    "!pip install mlxtend\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example 1: Basic Association Rule Mining\n",
    "\n",
    "Let's start with a simple dataset of transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample transaction data\n",
    "dataset1 = [['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "            ['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "            ['Milk', 'Apple', 'Kidney Beans', 'Eggs'],\n",
    "            ['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt'],\n",
    "            ['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs']]\n",
    "\n",
    "# Print the dataset\n",
    "print(\"Raw Dataset 1:\")\n",
    "for transaction in dataset1:\n",
    "    print(transaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data Preprocessing\n",
    "\n",
    "The Apriori algorithm expects data in a one-hot encoded format, where each row represents a transaction and each column represents an item. The value is `True` or `1` if the item is in the transaction, and `False` or `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dataset1).transform(dataset1)\n",
    "df1 = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "print(\"\\nOne-Hot Encoded DataFrame 1:\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Apply Apriori Algorithm\n",
    "\n",
    "Now, we apply the Apriori algorithm to find frequent itemsets. The `min_support` parameter specifies the minimum support threshold for an itemset to be considered frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find frequent itemsets with min_support = 0.6 (i.e., itemset appears in at least 60% of transactions)\n",
    "frequent_itemsets1 = apriori(df1, min_support=0.6, use_colnames=True)\n",
    "\n",
    "print(\"Frequent Itemsets (min_support=0.6):\")\n",
    "frequent_itemsets1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Generate Association Rules\n",
    "\n",
    "Once we have the frequent itemsets, we can generate association rules. We'll use `confidence` as the metric and set a minimum threshold (e.g., `min_threshold=0.7`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate association rules with min_confidence = 0.7\n",
    "rules1 = association_rules(frequent_itemsets1, metric=\"confidence\", min_threshold=0.7)\n",
    "\n",
    "print(\"Association Rules (min_confidence=0.7):\")\n",
    "rules1[['antecedents', 'consequents', 'support', 'confidence', 'lift']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Interpret the Results\n",
    "\n",
    "Let's look at one of the rules:\n",
    "- **Rule:** `{Onion} -> {Eggs}`\n",
    "- **Support:** This value (e.g., 0.6) means that 60% of all transactions contain both Onion and Eggs.\n",
    "- **Confidence:** If the confidence is 1.0, it means that 100% of the transactions that contain Onion also contain Eggs.\n",
    "- **Lift:** If the lift is, for example, 1.25, it means that customers are 1.25 times more likely to buy Eggs if they buy Onion, compared to if the purchase of Eggs was independent of the purchase of Onion. A lift > 1 indicates a positive correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example 2: Exploring Different Parameters and a slightly different view\n",
    "\n",
    "Let's use another small dataset and see how changing `min_support` and `min_threshold` for confidence affects the rules generated. This dataset is already in a list of lists format, suitable for `TransactionEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = [['bread', 'milk', 'butter'],\n",
    "            ['bread', 'butter', 'cheese', 'jam'],\n",
    "            ['milk', 'butter', 'cheese'],\n",
    "            ['bread', 'milk', 'jam'],\n",
    "            ['bread', 'milk', 'butter', 'cheese'],\n",
    "            ['tea', 'milk'],\n",
    "            ['bread', 'butter', 'jam']]\n",
    "\n",
    "te2 = TransactionEncoder()\n",
    "te_ary2 = te2.fit(dataset2).transform(dataset2)\n",
    "df2 = pd.DataFrame(te_ary2, columns=te2.columns_)\n",
    "\n",
    "print(\"One-Hot Encoded DataFrame 2:\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Experimenting with `min_support`\n",
    "\n",
    "Let's try a lower `min_support` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower min_support\n",
    "frequent_itemsets2_low_support = apriori(df2, min_support=0.2, use_colnames=True) # itemset appears in at least ~20% of transactions\n",
    "print(\"Frequent Itemsets (min_support=0.2):\")\n",
    "print(frequent_itemsets2_low_support)\n",
    "\n",
    "# Generate rules with min_confidence = 0.6\n",
    "rules2_low_support = association_rules(frequent_itemsets2_low_support, metric=\"confidence\", min_threshold=0.6)\n",
    "print(\"\\nAssociation Rules (min_support=0.2, min_confidence=0.6):\")\n",
    "rules2_low_support[['antecedents', 'consequents', 'support', 'confidence', 'lift']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try a higher `min_support`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher min_support\n",
    "frequent_itemsets2_high_support = apriori(df2, min_support=0.5, use_colnames=True) # itemset appears in at least 50% of transactions\n",
    "print(\"Frequent Itemsets (min_support=0.5):\")\n",
    "print(frequent_itemsets2_high_support)\n",
    "\n",
    "# Generate rules with min_confidence = 0.6\n",
    "rules2_high_support = association_rules(frequent_itemsets2_high_support, metric=\"confidence\", min_threshold=0.6)\n",
    "print(\"\\nAssociation Rules (min_support=0.5, min_confidence=0.6):\")\n",
    "rules2_high_support[['antecedents', 'consequents', 'support', 'confidence', 'lift']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "You should notice that a lower `min_support` results in more frequent itemsets and potentially more rules. A higher `min_support` leads to fewer, more common itemsets and rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Experimenting with `min_threshold` for Confidence\n",
    "\n",
    "Let's use the frequent itemsets from `min_support=0.2` and vary the `min_threshold` for confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using frequent_itemsets2_low_support (min_support=0.2)\n",
    "\n",
    "# Lower min_confidence\n",
    "rules2_low_confidence = association_rules(frequent_itemsets2_low_support, metric=\"confidence\", min_threshold=0.5)\n",
    "print(\"Association Rules (min_support=0.2, min_confidence=0.5):\")\n",
    "print(rules2_low_confidence[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
    "\n",
    "# Higher min_confidence\n",
    "rules2_high_confidence = association_rules(frequent_itemsets2_low_support, metric=\"confidence\", min_threshold=0.8)\n",
    "print(\"\\nAssociation Rules (min_support=0.2, min_confidence=0.8):\")\n",
    "rules2_high_confidence[['antecedents', 'consequents', 'support', 'confidence', 'lift']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "A lower `min_threshold` for confidence will generate more rules, including those where the antecedent doesn't strongly imply the consequent. A higher `min_threshold` will yield fewer, but stronger, rules where the implication is more certain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Task: Groceries Dataset\n",
    "\n",
    "Now it's your turn! We have a small dataset of grocery items from a few transactions. Your task is to perform association rule mining on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student dataset\n",
    "student_dataset = [\n",
    "    ['Apples', 'Bananas', 'Cereal'],\n",
    "    ['Milk', 'Bread', 'Butter'],\n",
    "    ['Apples', 'Bread', 'Eggs'],\n",
    "    ['Bananas', 'Milk', 'Cereal', 'Sugar'],\n",
    "    ['Apples', 'Milk', 'Bread', 'Butter'],\n",
    "    ['Coffee', 'Sugar', 'Cookies'],\n",
    "    ['Apples', 'Bananas', 'Bread'],\n",
    "    ['Milk', 'Cereal', 'Sugar'],\n",
    "    ['Apples', 'Bread', 'Butter', 'Cheese'],\n",
    "    ['Bananas', 'Cereal', 'Yogurt']\n",
    "]\n",
    "\n",
    "# Print the student dataset\n",
    "print(\"Student Dataset:\")\n",
    "for transaction in student_dataset:\n",
    "    print(transaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Tasks:\n",
    "\n",
    "1.  **Load and Preprocess the Data:**\n",
    "    * Use `TransactionEncoder` to transform `student_dataset` into a one-hot encoded pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Task 1\n",
    "\n",
    "\n",
    "print(\"One-Hot Encoded Student DataFrame:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  **Apply the Apriori Algorithm:**\n",
    "    * Find frequent itemsets using the Apriori algorithm. Choose a `min_support` value that you think is reasonable for this dataset (e.g., an itemset should appear in at least 2 or 3 transactions). Justify your choice briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Task 2\n",
    "# Justification for min_support:\n",
    "# The dataset has 10 transactions. A min_support of 0.2 means an itemset must appear in at least 10 * 0.2 = 2 transactions.\n",
    "# A min_support of 0.3 means an itemset must appear in at least 10 * 0.3 = 3 transactions.\n",
    "# Let's start with min_support = 0.2, as it's a small dataset and we want to see some initial patterns.\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Frequent Itemsets (min_support={min_support}):\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  **Generate Association Rules:**\n",
    "    * Generate association rules from the frequent itemsets. Choose a `min_threshold` for confidence (e.g., 0.5 or 0.6). Justify your choice briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Task 3\n",
    "# Justification for min_threshold (confidence):\n",
    "# A confidence of 0.5 means that in 50% of the cases where the antecedent is present, the consequent is also present.\n",
    "# For a small dataset, this might reveal some initial interesting rules without being too restrictive.\n",
    "\n",
    "min_confidence = 0.5\n",
    "\n",
    "print(f\"Association Rules (min_confidence={min_confidence}):\")\n",
    "rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.  **Identify and Interpret Interesting Rules:**\n",
    "    * From the generated rules, select 2-3 rules that you find interesting.\n",
    "    * For each selected rule, explain what it means in the context of the grocery data. Discuss its support, confidence, and lift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Double-click here to edit and write your interpretation for Task 4)*\n",
    "\n",
    "**Example Interpretation (you will pick your own rules from your results):**\n",
    "\n",
    "**Rule 1: {Antecedent} -> {Consequent}**\n",
    "* **Support:** [Value from your table] - This means that [Value]% of all transactions contain both {Antecedent} and {Consequent}.\n",
    "* **Confidence:** [Value from your table] - This means that [Value]% of the transactions that contain {Antecedent} also contain {Consequent}.\n",
    "* **Lift:** [Value from your table] - This means that customers are [Value] times more likely to buy {Consequent} if they buy {Antecedent}, compared to if the purchases were independent. (Interpret if >1, <1, or =1).\n",
    "* **Interestingness:** Why do you find this rule interesting in the context of grocery shopping?\n",
    "\n",
    "**Rule 2: ...**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.  **Experiment (Optional but Recommended):**\n",
    "    * Try changing your `min_support` and `min_threshold` (confidence) values. For example, make `min_support` lower or higher, and `min_threshold` for confidence lower or higher.\n",
    "    * How does this affect the number and type of rules generated? Briefly describe your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Task 5 (Optional)\n",
    "\n",
    "# Example: Lowering min_support further and keeping confidence moderate\n",
    "min_support_v2 = 0.1 # At least 1 transaction\n",
    "\n",
    "\n",
    "print(f\"\\n--- Experiment: min_support={min_support_v2}, min_confidence=0.5 ---\")\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets_v2)\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules_student_v2[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
    "\n",
    "# Example: Using original min_support (0.2) but increasing confidence\n",
    "min_confidence_v3 = 0.8\n",
    "\n",
    "\n",
    "print(f\"\\n--- Experiment: min_support={min_support}, min_confidence={min_confidence_v3} ---\")\n",
    "print(\"Frequent Itemsets (same as Task 2):\")\n",
    "print(frequent_itemsets)\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules_v3[['antecedents', 'consequents', 'support', 'confidence', 'lift']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Double-click here to edit and write your observations for Task 5)*\n",
    "\n",
    "**Student's Observations for Task 5:**\n",
    "\n",
    "* **Lowering `min_support` (e.g., to 0.1 from 0.2) while keeping `min_confidence` constant (e.g., at 0.5):**\n",
    "    \n",
    "    \n",
    "* **Increasing `min_confidence` (e.g., to 0.8 from 0.5) while keeping `min_support` constant (e.g., at 0.2):**\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "In this lab, we explored association rule mining using the Apriori algorithm. We learned how to:\n",
    "* Prepare data for association rule mining.\n",
    "* Use the `mlxtend` library to find frequent itemsets and generate rules.\n",
    "* Interpret key metrics like support, confidence, and lift.\n",
    "* Understand how parameters like `min_support` and `min_threshold` (for confidence) influence the outcome.\n",
    "\n",
    "Association rule mining is a powerful tool for uncovering hidden patterns in transactional data, with applications in retail, e-commerce, healthcare, and more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "342wi25",
   "language": "python",
   "name": "342wi25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
